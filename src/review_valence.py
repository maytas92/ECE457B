import json_reader
import valence_data
import input_membership_functions
import json
from collections import OrderedDict

# constant that denotes that
# a word is not found in the
# valence data set
DOES_NOT_EXIST = 10

# a constant that denotes that
# a 'pos' tag did not have
# a word with a 'max' valence
INIT_MAX_VALENCE = -10

# This class acts as a wrapper around the
# ReviewJsonReader and the ValenceData
# classes to score words based on the
# valence data set.
class ReviewValence:
	def __init__(self):
		self.review_json = json_reader.ReviewJsonReader(
                 './data/yelp_academic_dataset_review.json',
                 './output/yelp_review_output.json')
		self.review_json.open_input_file()
		self.review_json.read_input_file()
		self.valence_data = valence_data.ValenceData('./data/valence.txt')
		self.valence_data.process_data()
		self.input_mem_functions = input_membership_functions.InputMembershipFunctions()
		self.unique_non_valence_words = {}
		self.output_non_valence = open('./data/output_non_valence.txt', 'w+')
		# An array of review elements. Each review element contains
		# a map with a key for each 'POS tag' and its corresponding 
		# value to be the 'max' valence.
		# The 'max' valence is simply the maximum of the valences of
		# the words for each 'POS tag'. 
		self.output_pos_max_valence = []

	# Iterates through the first 'num_reviews'
	# For each review it looks up the valence
	# score for each word across all POS tags.
	def process_reviews(self, num_reviews):
		# Should return the first ten json records
		for i in range(num_reviews):
  			self.review_json.process_record()
  	# review_output is a map with the key being a POS Tag
  	# and the value an array of words falling into that
  	# POS Tag category.
		print "Iterating through review json data"
		for review_output in self.review_json.pos_tag_review_output:
			# Per review iterate through all the pos tags and associated
			# list of words
			word_with_valence_count = 0
			review_words_count = 0
			pos_tag_max_valence = OrderedDict()

			for pos_tag, words in review_output.items():
				# Check if word exists in the valence data
				max_valence = INIT_MAX_VALENCE
				for word in words:
					review_words_count += 1
					valence_score = self.get_valence_score(word)
					if valence_score != DOES_NOT_EXIST:
						if valence_score > max_valence:
							pos_tag_max_valence[pos_tag] = valence_score
						word_with_valence_count += 1
					else:
						if word not in self.unique_non_valence_words:
							self.unique_non_valence_words[word] = 1
						else:
							self.unique_non_valence_words[word] += 1
						#input_mem_functions
			print "Percent of words found in valence dataset ", word_with_valence_count / float(review_words_count)	* 100

			# Append the max_valence map data to the output array
			self.output_pos_max_valence.append(pos_tag_max_valence)

		print "Iterating through non-valence data"
		for word, count in self.unique_non_valence_words.items():
			try:
				self.output_non_valence.write(word + "=" + str(count) + '\n')
			except UnicodeEncodeError:
				print "Encoding error - proceeding with next word"

		print "Inferencing"
		# Iterate through the max_valence data and get inferencing!
		for review in self.output_pos_max_valence:
			for pos_tag, max_valence in review.items():
				print "pos_tag=", pos_tag, " max_valence=", max_valence

	# Valence scores from the data source lie between
	# [-5, 5]. It returns the score based on the 
	# map generated by the ValenceData object.
	def get_valence_score(self, word):
		if word in self.valence_data._data_map:
			return self.valence_data._data_map[word]
		return DOES_NOT_EXIST

############### MAIN ###############
rv = ReviewValence()
rv.process_reviews(int(1e2))
